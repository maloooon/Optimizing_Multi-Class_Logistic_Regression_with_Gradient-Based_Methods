{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Data Science 2024 Homework 1\n",
    "\n",
    "Students:\n",
    "\n",
    "Alberto Calabrese Nº:2103405\n",
    "\n",
    "Greta d'Amore Grelli Nº:2122424\n",
    "\n",
    "Eleonora Mesaglio Nº:2103402\n",
    "\n",
    "Marlon Helbing Nº:2106578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a seed for deterministic outputs\n",
    "SEED = 42\n",
    "np.random.seed(seed = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "NUM_FEATURES = 1000\n",
    "\n",
    "# Generate a 1000x1000 matrix with random samples from a standard normal distribution\n",
    "# This is our data matrix, which contains 1000 samples (rows) with 1000 features each (columns)\n",
    "# A MATRIX\n",
    "data_matrix = np.random.normal(0, 1, size = (NUM_SAMPLES, NUM_FEATURES)) # He refers to it as A\n",
    "A = data_matrix \n",
    "# Now 'data_matrix' contains random values drawn from N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 50\n",
    "\n",
    "# This is our weight matrix that we initialize like this ; these weights we want to learn\n",
    "# it has 1000 features (rows) with 50 labels each (columns)\n",
    "# X MATRIX\n",
    "weight_matrix = np.random.normal(0, 1, size = (NUM_FEATURES, NUM_LABELS)) # He refers to it as X \n",
    "X = weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "# This matrix is used to help generating our supervised gold labels \n",
    "# It is of size 1000 training examples (rows) and their labels (columns)\n",
    "generative_matrix = np.random.normal(0, 1, size = (NUM_EXAMPLES, NUM_LABELS)) # He refers to it as E \n",
    "E = generative_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AX + E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector with numbers from 1 to 50\n",
    "label_vector = np.arange(1, 51)\n",
    "\n",
    "# Print the vector\n",
    "#print(label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Now he wants us to calculate AX+E to generate labels for the 1000 training examples (such that we have a supervised learning set) +\n",
    "\n",
    "# Calculate the matrix product AX\n",
    "AX = np.matmul(data_matrix, weight_matrix)  # or simply: AX = A @ X\n",
    "\n",
    "# Add E to AX element-wise\n",
    "result_matrix = AX + generative_matrix\n",
    "\n",
    "print(result_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MAX INDEX AS CLASS LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 43 44  7 49 21 10 15 41 43 33 45  8 31 35 29 28 46 36 36  4 23 14  0\n",
      "  8 29 20 31  8  7 27 47 12 11 33  3  5 16  7 25 10 46 27 41 28  0  1  2\n",
      " 20 48 39 49 36 10 32  4 22 15 11 19 20 30 17 38 49 15 15 12 11  4  5 35\n",
      "  5  4 36  5 34 47 15  0 34  2 35 38 41 21 41 14 29 24  7  0 30  0  9 29\n",
      " 24 12 21 45 49 23  7 26 21  5 36 43 42 30 25 26 39  6 44 21 26 37  0 36\n",
      " 47 22  2 46  5  4 10 10 17  3 43  1 20  9 28 11  9 48 23 35 26 11 27 36\n",
      " 28 27 42 35 35 16 34 42 12  2  7 44 41 40 46 23 33  9  0 42 26 43 29 15\n",
      " 13  7  4  4 33 10  7 32 16 32 30 15 32 48 21  7 12 22 32 19 17 13 26 38\n",
      " 38  3 31 35 19 49 20 39 34 36  4 21 24 21 47 26 39 43 33 28 45 21 13 22\n",
      " 23 15 37 23 30  9 38 44 10  1 27 37 10 16 24  3 29 21  5 25 40 36 26 22\n",
      " 37 12 18 10 10 13  4 39 22 19 33 12 16 44 11 22 21 12  4 45 43 28  9 27\n",
      " 48 40 41 13 15 34 32 36 41 19 43 23  6  9 41 40 18 23 28 41 26 30 15 28\n",
      " 27 36 10 34 16 14  8 49  9 47 21 49 28  9 34 17 45 25 48  6 36 25 38 11\n",
      " 26 31 48 49 27 21 33  9  1  7 43 37 15 15 21 10 36  1  0 11 17 17 26 13\n",
      " 16  8 28 24 31 42 37 37  5 37 13 30 26 40  8 28 37 28 15 39 45 14 22 38\n",
      " 22 11  4 43  3 41 44 29 32 27  3  9  9 18 45 18 22 17 20 41 27 32  7  5\n",
      " 22 33 46  7 47 43  6 13 40 44 22 23  9  2 23 17 10 10 44 22 29 38 12 33\n",
      "  1 23 33 41  3 19 10  6 20 25 15 30 36 15 33  0 12  7 31 35  2  4  2  3\n",
      " 34 42 46 22 10 30 25 28 18  2 42 33 21 33  5 16  0  5 15 12 44 46 26 34\n",
      " 18 34 10 17 30  7 44 24 33 47 35  3 22  4 28 14 33 16  9 41 30 20 18 44\n",
      " 29 43  5 28 19 13 18  8 20 31  3 21 13 45  0 21  5  8  6 28 25 38 24 18\n",
      " 46 18 43 20 21 33  2 23 14 14 33  8 37 43  2  2  8 34 29  1 24 20  6  5\n",
      " 34 10 44 13  1  3 21 44 16 15 34 47 49 12 30 36  3 21  7  3 46 25 14  7\n",
      " 24 40 38 25 22  8 34  7 12 48 36 38  8 12 28 33 42 19  0  6 20 10 34 13\n",
      " 24  6 17 42 31 12  9 12 17  6 45  7  6 47  9 15 32 37  4 32 21 18 10 41\n",
      " 31 49  2 16 29 46 12 24  2  7 48 34 15 22 26 12 39 49 25 30 35 18 36  9\n",
      " 28 43 19 45 23 25 34 20  0 20 38 37 36 36  3 30  3 33 37 14 34  8 31 29\n",
      " 17 28  3 19 14  7 33 26 32 43 41 14 30 34 18 14  4 18 38 15  3 48 11 38\n",
      " 39 36 33 36  8 47 43 30 48 30  6  9 22 14  8 43  3  9 19  4  2 40 46 31\n",
      "  4 49 26 48 44 43 35 23 19 34 39 19 29 40 27 18 31 45 43 15 42 23 47 15\n",
      " 32 46 32 22 17 43 19 30  9 28 16 12 44 31 47 23 16 24 18  0 34 38 29  4\n",
      "  1 34 47  2 20 43 29 36 39 15 16 48 47 22 46 28 46 46 37  7 26 33 33 11\n",
      "  4 22 18 10 22  5 34 49 11 27 45 48 12 33 28 19 15 17 41 33 25 46 13 27\n",
      "  9  1 26 33 43  8 32 26  0 36  0 28  5 10  4 48 16 39 43 36 27  6 26 42\n",
      " 12 37 41 24 33  9 15 29 36 24  2 44  2 20 24 15 21 21  2  4 36 32 29  2\n",
      "  0 21 32 31 46  9 23 30 35 17 43 23  9  7 35  5 29 36 25 46 33 14 30 17\n",
      " 14 48 28 23 38 10 41 16 13 18 23 30  0 16 29 14 20  1  7 29 31  2 35 17\n",
      " 47 18 26 43  0 21 14 16 11 15 23  5 27 14  0  1 33 31 33  7  2 37  1 38\n",
      "  4 40  5 27 17 36 32 37  5  6 48 49 28 14 37  6 47  7 21 22 11  5 25 13\n",
      " 38 13 32 16 28 34 25  2 37 18 26 18 14  8 20 22 29 34 23 41 38 22 37 26\n",
      " 38  1  6 34 38 47 10 13 42  7  0 11 37 49 19 37  8 35 23 47 45 19 22 16\n",
      " 42 44 16 42  0 34  4  9 44  1 40 18 40 10 45 49]\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# We find our labels by considering the max index in the row as the class label\n",
    "\n",
    "# Find the column indices of maximum values for each row\n",
    "labels = np.argmax(result_matrix, axis=1)\n",
    "\n",
    "print(labels)\n",
    "\n",
    "#print(result_matrix[2,:])\n",
    "print(labels.shape)\n",
    "\n",
    "# 'max_indices' now contains the column indices of maximum values for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the negative log-likelihood function\n",
    "def cost_func(data_matrix, weight_matrix, labels):\n",
    "    scores = np.dot(data_matrix, weight_matrix)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    corect_logprobs = -np.log(probs[range(NUM_EXAMPLES), labels])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    return data_loss\n",
    "\n",
    "\n",
    "# Define the function to compute the gradient of the negative log-likelihood function\n",
    "def gradient(data_matrix, weight_matrix, labels):\n",
    "    scores = np.dot(data_matrix, weight_matrix)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    probs[range(NUM_EXAMPLES), labels] -= 1\n",
    "    dW = np.dot(data_matrix.T, probs)\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 4.146766\n",
      "iteration 100: loss 0.207946\n",
      "iteration 200: loss 0.116930\n",
      "iteration 300: loss 0.082825\n",
      "iteration 400: loss 0.064673\n",
      "iteration 500: loss 0.053311\n",
      "iteration 600: loss 0.045492\n",
      "iteration 700: loss 0.039762\n",
      "iteration 800: loss 0.035373\n",
      "iteration 900: loss 0.031897\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate and the number of iterations\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(num_iterations):\n",
    "    grad = gradient(data_matrix, weight_matrix, max_indices)\n",
    "    weight_matrix -= learning_rate * grad\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, cost_func(data_matrix, weight_matrix, max_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELED = 500\n",
    "Y_0 = np.random.rand(NUM_SAMPLES, NUM_LABELS) # define an appropriate starting point\n",
    "assert Y_0.shape == (NUM_SAMPLES, NUM_LABELS)\n",
    "\n",
    "EPSILON = 1e-6 # define small epsilon for stopping criterion\n",
    "MAX_ITER = 2000 # and/or a maximum number of iterations (or even a maximum time)\n",
    "\n",
    "ALPHA = 0.01 # define a fixed stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "Y_iterates = [Y_0]\n",
    "times = [0]\n",
    "start = time.time()\n",
    "\n",
    "grad = gradient(data_matrix, weight_matrix, max_indices)\n",
    "while len(Y_iterates) < MAX_ITER and np.linalg.norm(grad) > EPSILON: # TO DO: write the condition for the while loop\n",
    "    new_y = Y_iterates[-1] - ALPHA * grad # write the update\n",
    "    Y_iterates.append(new_y)\n",
    "    times.append(time.time() - start)\n",
    "    # Check the stopping criterion\n",
    "    grad = gradient(data_matrix, weight_matrix, max_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (1000,) (1000,50) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(times, [cost_func(data_matrix, weight_matrix, y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y_iterates])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU time (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjective function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(times, [cost_func(data_matrix, weight_matrix, y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y_iterates])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU time (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjective function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mcost_func\u001b[0;34m(data_matrix, weight_matrix, labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m exp_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(scores)\n\u001b[1;32m      5\u001b[0m probs \u001b[38;5;241m=\u001b[39m exp_scores \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exp_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m corect_logprobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(probs[\u001b[38;5;28mrange\u001b[39m(NUM_EXAMPLES), labels])\n\u001b[1;32m      7\u001b[0m data_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(corect_logprobs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_loss\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (1000,) (1000,50) "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(times, [cost_func(data_matrix, weight_matrix, y.astype(int)) for y in Y_iterates])\n",
    "plt.xlabel('CPU time (seconds)')\n",
    "plt.ylabel('Objective function')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BCGD with Randomized Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define as a block a single column in the parameter matrix X. Thus, one block defines all features for a single class. As this is a column vector in the matrix X, our partial gradient is now only dependent on c (because we have a gradient for all the features of one class ).\n",
    "\n",
    "\n",
    "Our partial derivative for one block then looks like the following\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(X)}{\\partial X_{c}} = - A^{T} * (L^{I} - Q) = A^{T} * (- L^{I} + Q)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$A$ has form $m x d$ ; it is our given matrix A.\n",
    "\\\n",
    "$L^{I}$ has form $m x 1$ ; it is the indicator vector containing 1's only at the positions where the label of the current sample i is equal to c, else it is 0\n",
    "$$\n",
    "L_{i}^{I}=\\begin{cases}\n",
    "\t\t\t1, & \\text{if $label_{i} = c $}\\\\\n",
    "            0, & \\text{otherwise}\n",
    "\t\t \\end{cases}\n",
    "$$\n",
    "\\\n",
    "$Q$ has form $m x 1$ ; it is the vector calculating the exponential expression $\\frac{e^{x_{c}^{T}a_{i}}}{\\sum_{c' = 1}^{k} e^{x_{c'}^{T}a_{i}} }$ for each sample $i$\n",
    "\n",
    "Thus, our result will be of form $d x 1$\n",
    "\n",
    "Note that the calculations needed for $L^{I}$ and $Q$ only depend on $c$ and $i$ . However, as we know all the samples $i$, we construct the vectors $L^{I}$ and $Q$ which are then only dependent on $c$ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### NOT TESTED YET ########\n",
    "m = 1000 # samples\n",
    "d = 1000 # features\n",
    "k = 50   # labels\n",
    "\n",
    "def partial_gradient(c):\n",
    "\n",
    "\n",
    "    # We define the partial gradient\n",
    "    \n",
    "    # Calculating indicator vector L \n",
    "\n",
    "    # Initialize empty L in size of all samples (=1000)\n",
    "    L = np.zeros(m,)\n",
    "\n",
    "    # Iterate over labels of each sample\n",
    "    for idx,label in enumerate(labels):\n",
    "        # If there is a label match\n",
    "        if label == c:\n",
    "            # We assign a 1\n",
    "            L[idx] = 1\n",
    "        # If there is no match\n",
    "        else:\n",
    "            # We assign a 0\n",
    "            L[idx] = 0\n",
    "\n",
    "    # Calculating vector Q\n",
    "\n",
    "    # Initialize empty Q in size of all samples (=1000)\n",
    "    \n",
    "    Q = np.zeros(m,)\n",
    "\n",
    "    # Iterate over all samples\n",
    "    for curr_sample in range(m):\n",
    "        nominator = np.exp((X[:,c]).T * A[curr_sample,:])\n",
    "        denominator = 0\n",
    "        # Iterate over all labels for the denominator\n",
    "        for curr_label in range(k):\n",
    "            denominator += np.exp((X[:,curr_label]).T * A[curr_sample,:])\n",
    "    \n",
    "        Q[curr_sample] = nominator/denominator\n",
    "\n",
    "\n",
    "    return A.T * ((-1 * L) * Q)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
